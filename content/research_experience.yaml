# ──────────────────────────────────────────────────────────────────
# RESEARCH EXPERIENCE — academic / research roles
# ──────────────────────────────────────────────────────────────────
# Each entry can have subsections (project groupings with headings).
#
# Fields used by both CVs:
#   role, company, dates, location, description, subsections
#
# Fields used only by the designed CV:
#   project_title    — bold heading (falls back to role if omitted)
#   designed_subtitle — line below the heading
#                       (auto-built from company + role + dates if omitted)

entries:
  - role: "MSc Researcher --- AI and Digital Health"
    company: "University of Westminster"
    dates: "Sep 2024 -- Present"
    location: "London, UK"
    project_title: "Scientific Literature Processing Platform"
    designed_subtitle: "University of Westminster, MSc AI & Digital Health | 2024--Present"
    description: >-
      Integrated platform for ingesting, structuring, validating, and indexing
      scientific literature at scale. Designed as a single deterministic system
      with reproducibility, provenance, and hardware efficiency as architectural
      requirements.
    subsections:
      - heading: "Platform Architecture & Orchestration"
        bullets:
          - "23-stage deterministic pipeline orchestrated via Prefect with distributed execution through RabbitMQ worker queues and hardware-aware compute profiles"
          - "6,636 documents / 100,034 pages at 99.91% success, producing contract-valid structured JSON with full provenance (manifests, hashing, structured logging, stage-level contracts)"
          - "Sustained dual NVIDIA B200 GPUs at 90--96% utilisation and CPUs at 56--65% for 52 continuous hours, generating ~1 TB across 2 million files with zero pipeline failure"
          - "181 Docker containers and 11 NLP models concurrently; all hyperparameters grid-searched with confusion matrix validation for high F1. No models fine-tuned---performance achieved through architectural discipline"

      - heading: "Ontology & Search Layer"
        bullets:
          - "Parsed and normalised biomedical ontologies (MONDO, HPO, EFO) into queryable graph structures (DuckDB) for defensible disease labelling and filtering"
          - "Parallel-processing ETL to index semantically chunked documents and extracted entities into Elasticsearch and PostgreSQL for high-recall retrieval and analytics"

      - heading: "Human-in-the-Loop Validation & Quality Assurance"
        bullets:
          - "SapBERT embedding similarity to propose and rank candidate feature aliases, with Flask application serving ML-ranked suggestions for expert approval/rejection"
          - "Evaluation methodology using stratified sampling and confusion matrices to quantify extraction and linking accuracy and guide thresholding"
          - "Outputs feed verified training datasets for downstream model evaluation---closing the loop from raw document to validated, reusable knowledge"

      - heading: "UK Biobank Large-Scale Data Extraction"
        bullets:
          - "Faculty stated extracting the full UK Biobank feature set across the 500,000-participant cohort was impossible. Read the entire DNA Nexus manual and identified a viable extraction path"
          - "Actual feature space was approximately 50,000 (not the assumed 10,000) including all arrays and instances. Pulled the complete dataset---50,000 features × 500,000 participants---into 15 parquet files totalling 42 GB"
          - "Optimised extraction from 15 hours to 30 minutes---a 30× improvement through architectural understanding of the platform, not brute compute"
